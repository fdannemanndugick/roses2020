{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROSES Unit 7 Machine Learning\n",
    "## Dr. Zachary Ross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest of supervised learning algorithms are linear models. We'll start by looking at linear classifiers, which are suitable for datasets where the classes are fully separable by a plane, such as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samp_class = 100\n",
    "c = 3\n",
    "d = 2\n",
    "center = 3\n",
    "\n",
    "X1 = np.column_stack([np.ones(n_samp_class), np.random.normal(0, 1, size=n_samp_class), np.random.normal(center, 1, size=n_samp_class)])\n",
    "X2 = np.column_stack([np.ones(n_samp_class), np.random.normal(-center, 1, size=n_samp_class), np.random.normal(-center, 1, size=n_samp_class)])\n",
    "X3 = np.column_stack([np.ones(n_samp_class), np.random.normal(center, 1, size=n_samp_class), np.random.normal(-center, 1, size=n_samp_class)])\n",
    "Y1 = np.zeros(X1.shape[0])\n",
    "Y2 = np.ones(X2.shape[0])\n",
    "Y3 = np.ones(X3.shape[0])*2\n",
    "\n",
    "X = np.concatenate([X1, X2, X3])\n",
    "Y = np.concatenate([Y1, Y2, Y3])\n",
    "idx = np.arange(Y.shape[0])\n",
    "X = X[idx,:]\n",
    "Y = Y[idx].astype(np.int)\n",
    "\n",
    "idx = np.where(Y==0)\n",
    "plt.scatter(X[idx,1], X[idx,2], c='r')\n",
    "idx = np.where(Y==1)\n",
    "plt.scatter(X[idx,1], X[idx,2], c='k')\n",
    "idx = np.where(Y==2)\n",
    "plt.scatter(X[idx,1], X[idx,2], c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This condition is called linear separability. An example of a dataset that is not linearly separable is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "D = 2\n",
    "K = 5\n",
    "X = np.zeros((N*K,D))\n",
    "y = np.zeros(N*K)\n",
    "for j in range(K):\n",
    "    ix = range(N*j,N*(j+1))\n",
    "    r = np.random.uniform(j+0.2, j+0.6, size=N)\n",
    "    theta = np.random.uniform(0, 360, size=N)\n",
    "    X[ix] = np.c_[r*np.sin(theta), r*np.cos(theta)]\n",
    "    y[ix] = j\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, edgecolor='k', cmap=plt.cm.rainbow)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll later consider models suitable for these types of datasets. For now, we'll focus on linear classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the dataset. It's stored in a Numpy NPZ file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.load(\"seismograms.npz\")\n",
    "X = f['X']\n",
    "Y = f['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the inputs, x, are called features, while the values to predict, y, are called targets or labels. Let's inspect the dataset to get a sense for what the different traces look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "t = np.arange(X.shape[1]) * dt\n",
    "for i in range(6):\n",
    "    fig, ax = plt.subplots(3, 1, sharex=True, sharey=True, figsize=(8,6))\n",
    "    if Y[i] == 0:\n",
    "        color = 'k'\n",
    "        label = 'P-wave'\n",
    "    elif Y[i] == 1:\n",
    "        color = 'r'\n",
    "        label = 'S-wave'\n",
    "    elif Y[i] == 2:\n",
    "        color = 'b'\n",
    "        label = 'Noise'\n",
    "    for j in range(3):\n",
    "        ax[j].plot(t, X[i,:,j], c=color, lw=1, label=label)\n",
    "    ax[2].set_xlabel(\"Time (sec)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a mixture of 3 signal types: P-waves, S-waves, and noise seismograms. Since we have ground truth labels for all waveforms, we can train a classifier to predict the correct class. With the types of models we will consider here (shallow networks), they often require you to pre-process the data to engineer good features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use spectral amplitudes for this tutorial, rather than the full time series\n",
    "X = np.fft.rfft(X, axis=1)\n",
    "X = np.abs(X)\n",
    "freq = np.fft.rfftfreq(400, d=0.01)\n",
    "\n",
    "# We'll preprocess the data by taking the logarithm\n",
    "X = np.log10(X)\n",
    "\n",
    "# And then it's common to normalize the data\n",
    "X -= np.mean(X, axis=0)[None,:,:]\n",
    "X /= np.std(X, axis=0)[None,:,:]\n",
    "\n",
    "# Here we are going to restrict ourselves to 20 Hz and less\n",
    "idx = np.where(freq<=20.0)[0]\n",
    "X = X[:,idx,1]\n",
    "\n",
    "num_features = X.shape[1]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to get this into a form that Pytorch can easily work with. We'll define a Dataset that stores our features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WfDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.from_numpy(features).float()\n",
    "        self.labels = torch.from_numpy(labels).long()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we build a waveform dataset and get the number of data samples (seismograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WfDataset(X, Y)\n",
    "n_samples = len(dataset)\n",
    "print(n_samples, \"seismograms in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning, a portion of the dataset is typically set aside for independent cross-validation. Here we will use 25% of the seismograms for our validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val = int(0.25*n_samples)\n",
    "indices = list(range(n_samples))\n",
    "\n",
    "# Randomly select seismograms for the validation set\n",
    "validation_idx = np.random.choice(indices, size=n_val, replace=False)\n",
    "train_idx = list(set(indices) - set(validation_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is designed to work easily with DataLoader objects, which automatically produce batches of data that can be iteratively processed during training. Let's create a DataLoader for both the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    sampler=train_sampler,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    sampler=validation_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model that we will consider is a linear softmax classifier. The model has the form $y = wx + b$, where $w$ is called the weight matrix (dims: 3x400), and $b$ is the bias vector (dims: 3x1). Given an input seismogram, it will output a vector $y$ (dims: 3x1), which represent the probabilities of the signal belonging to each respective class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models in pytorch are defined by classes. \n",
    "class LinearSoftmaxClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(LinearSoftmaxClassifier, self).__init__()        \n",
    "        \n",
    "        # This is where our linear model is defined. The inputs have dimension 400 (the number of time steps), and the output, y, has dimension 3\n",
    "        self.layer1 = torch.nn.Linear(num_features, num_classes)\n",
    "        \n",
    "    # This function defines what happens when you input a seismogram, x\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # This is our linear classifier equation, y = wx + b\n",
    "        y = self.layer1(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we instantiate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSoftmaxClassifier(num_features=num_features, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, $w$ and $b$ in our model are uninitialized parameters. The goal is to learn optimal parameter values that minimize our prediction error against the ground truth. The process by which these parameters are learned is called 'training'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks (which includes our linear softmax classifier) are typically trained with gradient descent algorithms, which take the form:\n",
    "\n",
    "$\\theta^{i+1} = \\theta^{i} - \\eta \\nabla_\\theta L$,\n",
    "\n",
    "where $\\theta^i$ represents the full set of parameters at iteration $i$, $\\eta$ is the learning rate (step size), and $L$ is an objective function called the loss. To perform gradient descent, we first pass some seismograms through the model and make class predictions for each. Then, $L$ is calculated by measuring the prediction error against the ground truth. Since we want to minimize this prediction error, we calculate the gradient $\\nabla_\\theta L$ with respect to $\\theta$, which is used in the above equation to determine the parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a gradient descent optimizer. lr is the learning rate. We provide the optimizer with a list of all model parameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define our loss function, $L$. For this problem, we'll use the cross-entropy loss, which is equivalent to the likelihood of our model correctly predicting the true class of a batch of seismograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our network. We'll perform gradient descent by iterating over the dataset and presenting batches of seismograms to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, lr, num_epochs=100, weight_decay=0):\n",
    "\n",
    "    training_acc = []\n",
    "    validation_acc = []\n",
    "    \n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Each epoch is defined by one full cycle through the dataset\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        categorical_accuracy = 0\n",
    "        count = 0\n",
    "        tmp_loss = []\n",
    "        \n",
    "        # This iterates over mini-batches of seismograms until the whole dataset has been seen\n",
    "        for data in train_loader:\n",
    "\n",
    "            x, y_true = data\n",
    "\n",
    "            # Need to reset the gradients for each iteration\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass for class predictions\n",
    "            y_pred = model(x)\n",
    "\n",
    "            # Calculate prediction error\n",
    "            L = loss(y_pred, y_true)\n",
    "\n",
    "            # Now, call autograd, which calculates the gradients of L w.r.t the parameters\n",
    "            L.backward()\n",
    "\n",
    "            # Perform parameter update\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store accuracy for later\n",
    "            y_pred_labels = torch.argmax(y_pred, dim=1)\n",
    "            categorical_accuracy += (y_pred_labels == y_true).sum().item()\n",
    "            count += y_true.shape[0]\n",
    "            \n",
    "            tmp_loss.append(L.item())\n",
    "\n",
    "        training_acc.append(categorical_accuracy/count)\n",
    "        training_loss.append(np.mean(tmp_loss))\n",
    "        \n",
    "        categorical_accuracy = 0\n",
    "        count = 0\n",
    "        tmp_loss = []\n",
    "        \n",
    "        # Now let's do this again for the validation dataset. This time, we don't need to calculate the gradients since we are not updating the parameters.\n",
    "        for data in val_loader:\n",
    "\n",
    "            x, y_true = data\n",
    "\n",
    "            # Forward pass for class predictions\n",
    "            y_pred = model(x)\n",
    "\n",
    "            # Calculate prediction error\n",
    "            L = loss(y_pred, y_true)\n",
    "\n",
    "            # Store accuracy for later\n",
    "            tmp_loss.append(L.item())\n",
    "            y_pred_labels = torch.argmax(y_pred, dim=1)\n",
    "            categorical_accuracy += (y_pred_labels == y_true).sum().item()\n",
    "            count += y_true.shape[0]\n",
    "            \n",
    "        validation_acc.append(categorical_accuracy/count)\n",
    "        validation_loss.append(np.mean(tmp_loss))\n",
    "        \n",
    "        if epoch % 10 ==0:\n",
    "            print(\"Epoch\", epoch, \"finished. Validation accuracy:\", validation_acc[-1])\n",
    "\n",
    "    return training_acc, validation_acc, training_loss, validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSoftmaxClassifier(num_features=num_features, num_classes=3)\n",
    "out = train(train_loader, val_loader, model, lr=1e-4, num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to look at the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(out):\n",
    "    training_acc, validation_acc, training_loss, validation_loss = out\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2,sharex=True, figsize=(16,6))\n",
    "    ax[0].plot(np.arange(len(training_acc)), training_loss, c='k', label=\"Training\")\n",
    "    ax[0].plot(np.arange(len(validation_acc)), validation_loss, c='b', label=\"Validation\")\n",
    "    ax[0].set_xlabel(\"Iteration\")\n",
    "    ax[0].set_ylabel(\"Loss\")\n",
    "    ax[0].legend(loc='upper right')\n",
    "    ax[0].set_xlim((0, None))\n",
    "    \n",
    "    ax[1].plot(np.arange(len(training_acc)), training_acc, c='k', label=\"Training\")\n",
    "    ax[1].plot(np.arange(len(validation_acc)), validation_acc, c='b', label=\"Validation\")\n",
    "    ax[1].set_xlabel(\"Iteration\")\n",
    "    ax[1].set_ylabel(\"Accuracy\")\n",
    "    ax[1].set_ylim((0, 1))\n",
    "    ax[1].set_xlim((0, None))\n",
    "    ax[1].legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_results(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training accuracy of the model increases iteratively during the training process and eventually converges to its optimal value. The validation accuracy is close to the training accuracy at all epochs, which tells us that we are not overfitting on the data. Overall, the accuracy of our model is about 53%, which is much better than randomly guessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the learning rate has a significant influence on the convergence properties. Linear models are convex and are guaranteed to converge, but may take longer or faster depending on the learning rate. Try adjusting the learning rate to 1e-2, 1e-1, and 1e-3 and repeating this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "model = LinearSoftmaxClassifier(num_features=num_features, num_classes=3)\n",
    "out = train(train_loader, val_loader, model, lr, num_epochs=100)\n",
    "plot_results(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this doesn't really improve the model's performance much. The main reason is that  The goal is then to learn a transformation of the data into a space where the classes are linearly separable.\n",
    "\n",
    "One powerful class of models to do this, which extends our linear classifier model, is the artificial neural network. Neural networks are layered systems that sequentially feed the outputs of each layer into the next layer. For example, a simple two-layer neural network looks like this:\n",
    "\n",
    "$y = W_2\\varphi(W_1x + b_1) + b_2$\n",
    "\n",
    "where $f = W_1x + b_1$ is exactly the linear model described before, and $\\varphi(f)$ is some non-linear function called an activation function. Here, $f$ is input to a second linear model with different parameters, after applying the activation function $\\varphi(f)$. Common examples of $\\varphi(f)$ include the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.linspace(-5, 5, 1000)\n",
    "sigmoid = lambda x: np.exp(x) / (1 + np.exp(x))\n",
    "fig = plt.figure()\n",
    "plt.plot(f, sigmoid(f))\n",
    "plt.xlabel(\"f\")\n",
    "plt.ylabel(\"Activation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need $\\varphi(f)$ in the first place? It has been proven mathematically that for certain classes of activation functions, a neural network can approximate any non-linear function (provided it has certain properties). Without it, our model can only approximate linear functions, no matter how many layers we have.\n",
    "\n",
    "Therefore $z = \\varphi(W_1x + b_1)$ represents a learned non-linear mapping into a new space, $z$, which is (hopefully) linearly separable. The function $\\varphi(f)$ has been chosen carefully such that our model can learn an arbitrary non-linear mapping. A simple example of such a model is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNeuralNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features=400, num_classes=3, num_hidden=40):\n",
    "        super(TwoLayerNeuralNetwork, self).__init__()        \n",
    "        \n",
    "        # This is slightly modified from before. The inputs have dimension 400 (the number of time steps), and the output, f, has dimension num_hidden\n",
    "        self.layer1 = torch.nn.Linear(num_features, num_hidden)\n",
    "        \n",
    "        # Unlike with the linear classifier, the 2-layer NN has a second layer that receives the outputs of layer 1. The number of outputs for this layer is now 3.\n",
    "        self.layer2 = torch.nn.Linear(num_hidden, num_classes)\n",
    "        \n",
    "    # This function defines what happens when you input a seismogram, x\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # This is our linear classifier equation, z = wx + b\n",
    "        f = self.layer1(x)\n",
    "        \n",
    "        # However to achieve a non-linear mapping function, we need to apply a non-linear activation to the outputs.\n",
    "        z = torch.relu(f)\n",
    "        \n",
    "        # Now, we will have transformed x into z, where z is linearly separable. Next we can use our linear softmax classifier:\n",
    "        out = self.layer2(z)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new layer, $z$, that we have added to this model (compared with the linear model), is referred to as a hidden layer, since the values are not directly observed as either inputs, or outputs. Unlike in the linear model, where the dimensions of $W$ and $b$ are determined by $x$ and $y$, the dimensions of $W_1$ and $b_1$ in a neural network are user-specified hyperparameters. Let's build a neural network with 10 hidden units, i.e. $dim(W_1)=(10x400)$ and $dim(b_1)=(10x1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train this model using the function we previously defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNeuralNetwork(num_features=num_features, num_classes=3, num_hidden=40)\n",
    "out = train(train_loader, val_loader, model, lr=5e-4, num_epochs=100)\n",
    "plot_results(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of this simple neural network is slightly better than what we got with the linear classifier. However modern deep neural networks can achieve far better performance and are composed of potentially hundreds of layers.\n",
    "\n",
    "Besides the performance, neural networks are quite different than the linear classifer that we trained before. In particular, the loss for a neural network is non-convex, which makes training much harder. Try adjusting the learning rate below to 1e-2, 1e-3, and 1e-5 and see how the training results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNeuralNetwork(num_features=num_features, num_classes=3, num_hidden=40)\n",
    "out = train(train_loader, val_loader, model, lr=1e-2, num_epochs=100)\n",
    "plot_results(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of hidden units has a substantial influence on the performance of the neural network because it controls the amount of detail that can be learned. See what happens to our performance (and the training time) when we increase this to 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "model = TwoLayerNeuralNetwork(num_features=num_features, num_classes=3, num_hidden=100)\n",
    "out = train(train_loader, val_loader, model, lr, num_epochs=100)\n",
    "plot_results(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about if we add a third layer to the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNeuralNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features=400, num_classes=3, num_hidden=40):\n",
    "        super(ThreeLayerNeuralNetwork, self).__init__()        \n",
    "        \n",
    "        # This is slightly modified from before. The inputs have dimension 400 (the number of time steps), and the output, f, has dimension num_hidden\n",
    "        self.layer1 = torch.nn.Linear(num_features, num_hidden)\n",
    "        \n",
    "        # Unlike with the linear classifier, the 2-layer NN has a second layer that receives the outputs of layer 1. The number of outputs for this layer is now 3.\n",
    "        self.layer2 = torch.nn.Linear(num_hidden, num_hidden)\n",
    "        \n",
    "        self.layer3 = torch.nn.Linear(num_hidden, num_classes)\n",
    "        \n",
    "    # This function defines what happens when you input a seismogram, x\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # This is our linear classifier equation, z = wx + b\n",
    "        f = self.layer1(x)\n",
    "        \n",
    "        # However to achieve a non-linear mapping function, we need to apply a non-linear activation to the outputs.\n",
    "        z = torch.relu(f)\n",
    "        \n",
    "        f = self.layer2(z)\n",
    "        \n",
    "        z = torch.relu(f)\n",
    "        \n",
    "        # Now, we will have transformed x into z, where z is linearly separable. Next we can use our linear softmax classifier:\n",
    "        out = self.layer3(z)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ThreeLayerNeuralNetwork(num_features=num_features, num_classes=3, num_hidden=40)\n",
    "out = train(train_loader, val_loader, model, lr=1e-2, num_epochs=100)\n",
    "plot_results(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's all for this tutorial!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
